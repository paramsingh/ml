{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size=65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([42, 30,  2, 50, 53, 19, 51, 30, 53, 30, 28, 33, 18, 25, 62, 26, 33,  9,\n",
       "        48,  2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open('input.txt').read()\n",
    "alphabet = set([c for c in data])\n",
    "stoi = {s:i for i,s in enumerate(alphabet)}\n",
    "itos = {value: key for key, value in stoi.items()}\n",
    "vocab_size = len(alphabet)\n",
    "print(f\"{vocab_size=}\")\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(encoded):\n",
    "    return ''.join([itos[i] for i in encoded])\n",
    "\n",
    "encoded = torch.tensor(encode(data), dtype=torch.long)\n",
    "encoded[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = int(0.9 * len(data))\n",
    "training_data = encoded[:x]\n",
    "validation_data = encoded[x:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "head_size = 16\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = training_data if split == 'train' else validation_data\n",
    "    indices = torch.randint(0, data.shape[0] - 1 - block_size, (batch_size,))\n",
    "    xs = torch.stack([data[i:i+block_size] for i in indices])\n",
    "    ys = torch.stack([data[i+1:i+block_size+1] for i in indices])\n",
    "    return xs, ys\n",
    "\n",
    "xbatch, ybatch = get_batch('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query, key, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single self attention head\n",
    "class Head(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(vocab_size, head_size)\n",
    "        self.key = nn.Linear(vocab_size, head_size)\n",
    "        self.value = nn.Linear(vocab_size, head_size)\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f'{x.shape=}, {vocab_size=}, {head_size=}')\n",
    "        q = self.query(x) # B, T, H\n",
    "        k = self.key(x) # B, T, H\n",
    "\n",
    "        print(f'{q.shape=}, {k.shape=}')\n",
    "\n",
    "\n",
    "        wei = q @ k.transpose(-1, -2) * (head_size ** -0.5) # B, T, T\n",
    "        print(f'{wei.shape=}, {self.mask.shape=}')\n",
    "        wei = wei.masked_fill(self.mask == 0, -float('inf')) # B, T, T\n",
    "        wei = F.softmax(wei, dim=-1) # B, T, T\n",
    "        v = self.value(x) # B, T, H\n",
    "        return wei @ v # B T T @ B T H -> B T H\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, vocab_size)\n",
    "        self.position_embeddings = nn.Embedding(block_size, vocab_size)\n",
    "        self.head = Head()\n",
    "        self.final = nn.Linear(head_size, vocab_size)\n",
    "    \n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        tokens_embeddings = self.token_embeddings(x) \n",
    "        position_embeddings = self.position_embeddings(torch.arange(x.shape[1]))\n",
    "        # print(f'{tokens_embeddings.shape=}, {position_embeddings.shape=}')\n",
    "        x = tokens_embeddings + position_embeddings\n",
    "\n",
    "        x = self.head(x) # (batch_size, block_size, head_size)\n",
    "        logits = self.final(x) # (batch_size, block_size, vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(B * T, C),\n",
    "                targets.view(B * T)\n",
    "            )\n",
    "        # print(f'{x.shape=}')\n",
    "        return loss, logits\n",
    "    \n",
    "    def generate(self, length):\n",
    "        result = ''\n",
    "        context = torch.randint(0, vocab_size, (1,1))\n",
    "        for _ in range(length):\n",
    "            print(f'{context.shape=}')\n",
    "            _, logits = self(context)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            sample = torch.multinomial(probs, 1, replacement=True)\n",
    "            context = torch.cat([context, sample], dim=1)\n",
    "            if context.shape[0] >= block_size:\n",
    "                context = context[1:]\n",
    "            result += itos[sample.item()]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context.shape=torch.Size([1, 1])\n",
      "x.shape=torch.Size([1, 1, 65]), vocab_size=65, head_size=16\n",
      "q.shape=torch.Size([1, 1, 16]), k.shape=torch.Size([1, 1, 16])\n",
      "wei.shape=torch.Size([1, 1, 1]), self.mask.shape=torch.Size([8, 8])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [1, 8] but got: [1, 1].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m m \u001b[39m=\u001b[39m LanguageModel()\n\u001b[0;32m----> 2\u001b[0m m\u001b[39m.\u001b[39;49mgenerate(\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[30], line 34\u001b[0m, in \u001b[0;36mLanguageModel.generate\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(length):\n\u001b[1;32m     33\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcontext\u001b[39m.\u001b[39mshape\u001b[39m=}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m     _, logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(context)\n\u001b[1;32m     35\u001b[0m     logits \u001b[39m=\u001b[39m logits[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m     36\u001b[0m     probs \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[30], line 16\u001b[0m, in \u001b[0;36mLanguageModel.forward\u001b[0;34m(self, x, targets)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# print(f'{tokens_embeddings.shape=}, {position_embeddings.shape=}')\u001b[39;00m\n\u001b[1;32m     14\u001b[0m x \u001b[39m=\u001b[39m tokens_embeddings \u001b[39m+\u001b[39m position_embeddings\n\u001b[0;32m---> 16\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead(x) \u001b[39m# (batch_size, block_size, head_size)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal(x) \u001b[39m# (batch_size, block_size, vocab_size)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[27], line 23\u001b[0m, in \u001b[0;36mHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m wei \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(wei, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# B, T, T\u001b[39;00m\n\u001b[1;32m     22\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(x) \u001b[39m# B, T, H\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[39mreturn\u001b[39;00m wei \u001b[39m@\u001b[39;49m v\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [1, 8] but got: [1, 1]."
     ]
    }
   ],
   "source": [
    "m = LanguageModel()\n",
    "m.generate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
