{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size=65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([31, 60, 50, 54, 64,  1, 21, 60, 64, 60, 37, 49, 19, 44, 41, 16, 49, 63,\n",
       "        53, 50])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open('input.txt').read()\n",
    "alphabet = set([c for c in data])\n",
    "stoi = {s:i for i,s in enumerate(alphabet)}\n",
    "itos = {value: key for key, value in stoi.items()}\n",
    "vocab_size = len(alphabet)\n",
    "print(f\"{vocab_size=}\")\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(encoded):\n",
    "    return ''.join([itos[i] for i in encoded])\n",
    "\n",
    "encoded = torch.tensor(encode(data), dtype=torch.long)\n",
    "encoded[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = int(0.9 * len(data))\n",
    "training_data = encoded[:x]\n",
    "validation_data = encoded[x:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "embedding_size = 64\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = training_data if split == 'train' else validation_data\n",
    "    indices = torch.randint(0, data.shape[0] - 1 - block_size, (batch_size,))\n",
    "    xs = torch.stack([data[i:i+block_size] for i in indices])\n",
    "    ys = torch.stack([data[i+1:i+block_size+1] for i in indices])\n",
    "    return xs, ys\n",
    "\n",
    "xbatch, ybatch = get_batch('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query, key, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single self attention head\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embedding_size, head_size)\n",
    "        self.key = nn.Linear(embedding_size, head_size)\n",
    "        self.value = nn.Linear(embedding_size, head_size)\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(f'{x.shape=}, {vocab_size=}, {head_size=}')\n",
    "        q = self.query(x) # B, T, H\n",
    "        k = self.key(x) # B, T, H\n",
    "\n",
    "        # print(f'{q.shape=}, {k.shape=}')\n",
    "\n",
    "        wei = q @ k.transpose(-1, -2) * (q.shape[2] ** -0.5) # B, T, T\n",
    "        # print(f'{wei.shape=}, {self.mask.shape=}')\n",
    "        T = wei.shape[1]\n",
    "        wei = wei.masked_fill(self.mask[:T, :T] == 0, -float('inf')) # B, T, T\n",
    "        wei = F.softmax(wei, dim=-1) # B, T, T\n",
    "        v = self.value(x) # B, T, H\n",
    "        # print(f'{wei.shape=}\\n\\n {v.shape=}')\n",
    "        return wei @ v # B T T @ B T H -> B T H\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(head_size, 4 * head_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * head_size, head_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.position_embeddings = nn.Embedding(block_size, embedding_size)\n",
    "        self.head = Head(embedding_size)\n",
    "        self.ff = FeedForward(embedding_size)\n",
    "        self.final = nn.Linear(embedding_size, vocab_size)\n",
    "    \n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        tokens_embeddings = self.token_embeddings(x) \n",
    "        position_embeddings = self.position_embeddings(torch.arange(x.shape[1]))\n",
    "        # print(f'{tokens_embeddings.shape=}, {position_embeddings.shape=}')\n",
    "        x = tokens_embeddings + position_embeddings\n",
    "\n",
    "        h = self.head(x) # (batch_size, block_size, head_size)\n",
    "        # print(f'{h.shape=} {x.shape=}')\n",
    "        x = x + h\n",
    "        x = x + self.ff(x) # (batch_size, block_size, head_size)\n",
    "\n",
    "        # print(x)\n",
    "        logits = self.final(x) # (batch_size, block_size, vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(B * T, C),\n",
    "                targets.view(B * T)\n",
    "            )\n",
    "        # print(f'{x.shape=}')\n",
    "        return loss, logits\n",
    "    \n",
    "    def generate(self, length):\n",
    "        result = ''\n",
    "        context = torch.randint(0, vocab_size, (1,1))\n",
    "        for _ in range(length):\n",
    "            # print(f'{context.shape=}')\n",
    "            _, logits = self(context)\n",
    "            # print(f'{logits.shape=}')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # print(f'{probs.shape=}')\n",
    "            sample = torch.multinomial(probs[:, -1], 1, replacement=True)\n",
    "            # print(f'{sample.shape=}, {sample.item()}')\n",
    "            context = torch.cat([context, sample.view(1, 1)], dim=1)\n",
    "            if context.shape[1] >= block_size:\n",
    "                context = context[:, 1:]\n",
    "            result += itos[sample.item()]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LSYCjUziHL'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LanguageModel()\n",
    "m.generate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 4.539271354675293\n",
      "step 1000 loss 2.3351659774780273\n",
      "step 2000 loss 2.2629077434539795\n",
      "step 3000 loss 2.236481189727783\n",
      "step 4000 loss 2.033236026763916\n",
      "step 5000 loss 1.9371086359024048\n",
      "step 6000 loss 2.062751531600952\n",
      "step 7000 loss 1.9947153329849243\n",
      "step 8000 loss 1.9462963342666626\n",
      "step 9000 loss 1.9698054790496826\n",
      "step 10000 loss 1.947810411453247\n",
      "step 11000 loss 1.9600310325622559\n",
      "step 12000 loss 1.7514913082122803\n",
      "step 13000 loss 1.8486696481704712\n",
      "step 14000 loss 1.8105295896530151\n",
      "step 15000 loss 1.8423959016799927\n",
      "step 16000 loss 2.106238603591919\n",
      "step 17000 loss 2.083470344543457\n",
      "step 18000 loss 1.79603910446167\n",
      "step 19000 loss 1.9162648916244507\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "losses = []\n",
    "\n",
    "for step in range(20000):\n",
    "    xbatch, ybatch = get_batch('train')\n",
    "    loss, _ = model(xbatch, targets=ybatch)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    losses.append(loss.item())\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f'step {step} loss {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETER:\n",
      "How, beance wake how\n",
      "dutes, by your wilkecond bed come.\n",
      "-save, king Rome uman: our that sly wervand him me me sinen envion:\n",
      "Ere, thee to thy lontagelast the did vaze me hem yo, saguends, ther bedard?\n",
      "\n",
      "SICAPULINA:\n",
      "Well world Cadamine\n",
      "Theaved give!\n",
      "I at's place!\n",
      "\n",
      "Look bogged-lerse out,\n",
      "As I mustould to make nect fear a and bears way gono.\n",
      "Fort no motheir\n",
      "of this purt. Tlook conce be that have thou to to a llice.\n",
      "Whaties my lose, as igonsuriefter highne,\n",
      "Brother, swooXson'd stage,\n",
      "Ippose wormentent lead compain'd will and to ye lies none in pride had\n",
      "Sore to cusenquon this sulserdly\n",
      "To starmy lords;\n",
      "Wheds. Here them I me!\n",
      "Be eoverss rece stie.\n",
      "\n",
      "LORD:\n",
      "Onchope.\n",
      "\n",
      "KING RICHORS:\n",
      "Pompon so\n",
      "no in the entsted sle! thought the but imps, should if thearstagento him:\n",
      "Comord's, o surves,\n",
      "March this a bany theave a daid bloods, will again, take swomnerys eno 'tward, lisheir of E VI:\n",
      "I at the by that fors conders upiting\n",
      "I my see grace,\n",
      "And womper: to lore endepeak: hanereou!\n",
      "He schoservy\n",
      "expassu\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 200\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            loss, _ = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(1.8799), 'val': tensor(2.0228)}"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
